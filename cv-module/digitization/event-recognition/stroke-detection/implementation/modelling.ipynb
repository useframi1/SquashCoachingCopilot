{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76098d92",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c3749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2916315a",
   "metadata": {},
   "source": [
    "## Loading Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1d2bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i want to put a json object in a pandas dataframe\n",
    "df_1 = pd.read_json(\n",
    "    \"/home/g03-s2025/Desktop/SquashCoachingCopilot/cv-module/digitization/event-recognition/stroke-detection/implementation/annotated_jsons/AO_Output.json\"\n",
    ")\n",
    "df_2 = pd.read_json(\n",
    "    \"/home/g03-s2025/Desktop/SquashCoachingCopilot/cv-module/digitization/event-recognition/stroke-detection/implementation/annotated_jsons/FA Output.json\"\n",
    ")\n",
    "df_3 = pd.read_json(\n",
    "    \"/home/g03-s2025/Desktop/SquashCoachingCopilot/cv-module/digitization/event-recognition/stroke-detection/implementation/annotated_jsons/video_4_annotated.json\"\n",
    ")\n",
    "df_4 = pd.read_json(\n",
    "    \"/home/g03-s2025/Desktop/SquashCoachingCopilot/cv-module/digitization/event-recognition/stroke-detection/implementation/annotated_jsons/integrated_data.json\"\n",
    ")\n",
    "df_5 = pd.read_json(\n",
    "    \"/home/g03-s2025/Desktop/SquashCoachingCopilot/cv-module/digitization/event-recognition/stroke-detection/implementation/annotated_jsons/nadines_video_annotated.json\"\n",
    ")\n",
    "df_6 = pd.read_json(\n",
    "    \"/home/g03-s2025/Desktop/SquashCoachingCopilot/cv-module/digitization/event-recognition/stroke-detection/implementation/annotated_jsons/video_1_annotated.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ccefcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add an index column to each dataframe\n",
    "df_1[\"index\"] = 1\n",
    "df_2[\"index\"] = 2\n",
    "df_3[\"index\"] = 3\n",
    "df_4[\"index\"] = 4\n",
    "df_5[\"index\"] = 5\n",
    "df_6[\"index\"] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cadbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df_1, df_2, df_3, df_4, df_5, df_6]\n",
    "for idx, df in enumerate(dfs, 1):\n",
    "    print(f\"df_{idx} event value counts:\")\n",
    "    print(df[\"event\"].value_counts())\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1797a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([df_5, df_2, df_1, df_4, df_6], ignore_index=True)\n",
    "test_df = df_3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7000160b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3030e434",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"event\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a66d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa4f1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"event\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4603d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_df(df):\n",
    "    expanded_df = (\n",
    "        df[\"keypoints\"]\n",
    "        .apply(\n",
    "            lambda person: {f\"x_{part}\": person[part][\"x\"] for part in person}\n",
    "            | {f\"y_{part}\": person[part][\"y\"] for part in person}\n",
    "        )\n",
    "        .apply(pd.Series)\n",
    "    )\n",
    "\n",
    "    # Merge back into original df if needed\n",
    "    df_expanded = pd.concat([df, expanded_df], axis=1)\n",
    "\n",
    "    # Optionally drop original nested column\n",
    "    df_expanded.drop(columns=[\"keypoints\"], inplace=True)\n",
    "    return df_expanded\n",
    "\n",
    "\n",
    "train_df_expanded = expand_df(train_df)\n",
    "test_df_expanded = expand_df(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d593a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_expanded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff9a098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_keypoints_df(df):\n",
    "    \"\"\"\n",
    "    Normalize all keypoints in the dataframe using body-relative normalization\n",
    "    REPLACES original x_, y_ columns with normalized values\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with columns: x_left_shoulder, y_left_shoulder, etc.\n",
    "\n",
    "    Returns:\n",
    "        df_normalized: DataFrame with normalized keypoint columns (same column names)\n",
    "    \"\"\"\n",
    "    df_norm = df.copy()\n",
    "\n",
    "    # Keypoint names\n",
    "    keypoint_names = [\n",
    "        \"left_shoulder\",\n",
    "        \"right_shoulder\",\n",
    "        \"left_elbow\",\n",
    "        \"right_elbow\",\n",
    "        \"left_wrist\",\n",
    "        \"right_wrist\",\n",
    "        \"left_hip\",\n",
    "        \"right_hip\",\n",
    "        \"left_knee\",\n",
    "        \"right_knee\",\n",
    "        \"left_ankle\",\n",
    "        \"right_ankle\",\n",
    "    ]\n",
    "\n",
    "    # Normalize frame by frame\n",
    "    for idx, row in df.iterrows():\n",
    "        # Calculate hip center\n",
    "        hip_center_x = (row[\"x_left_hip\"] + row[\"x_right_hip\"]) / 2\n",
    "        hip_center_y = (row[\"y_left_hip\"] + row[\"y_right_hip\"]) / 2\n",
    "\n",
    "        # Calculate shoulder center\n",
    "        shoulder_center_x = (row[\"x_left_shoulder\"] + row[\"x_right_shoulder\"]) / 2\n",
    "        shoulder_center_y = (row[\"y_left_shoulder\"] + row[\"y_right_shoulder\"]) / 2\n",
    "\n",
    "        # Calculate torso length\n",
    "        torso_length = np.sqrt(\n",
    "            (shoulder_center_x - hip_center_x) ** 2\n",
    "            + (shoulder_center_y - hip_center_y) ** 2\n",
    "        )\n",
    "\n",
    "        if torso_length < 1e-6:\n",
    "            torso_length = 1.0\n",
    "\n",
    "        # REPLACE original columns with normalized values\n",
    "        for name in keypoint_names:\n",
    "            df_norm.at[idx, f\"x_{name}\"] = (\n",
    "                row[f\"x_{name}\"] - hip_center_x\n",
    "            ) / torso_length\n",
    "            df_norm.at[idx, f\"y_{name}\"] = (\n",
    "                row[f\"y_{name}\"] - hip_center_y\n",
    "            ) / torso_length\n",
    "\n",
    "    return df_norm\n",
    "\n",
    "\n",
    "train_df_expanded = normalize_keypoints_df(train_df_expanded)\n",
    "test_df_expanded = normalize_keypoints_df(test_df_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd2c9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sequence(df_expanded, window = 15):\n",
    "    event_indices = df_expanded[df_expanded[\"event\"].notnull()].index\n",
    "\n",
    "    selected_indices = set()\n",
    "    neither_count = 0\n",
    "    MAX_NEITHER_SEGMENTS = 100\n",
    "    \n",
    "    # Step 1: Annotate labeled events and their 15-frame tails\n",
    "    for idx in event_indices:\n",
    "        label = df_expanded.at[idx, \"event\"]\n",
    "        group_id = df_expanded.at[idx, \"index\"]\n",
    "        selected_indices.add(idx)\n",
    "        df_expanded.at[idx, \"event\"] = label\n",
    "\n",
    "        for offset in range(0, window/2):\n",
    "            prev_idx = idx - offset\n",
    "            if prev_idx < 0:\n",
    "                break\n",
    "\n",
    "            if (\n",
    "                pd.notnull(df_expanded.at[prev_idx, \"event\"])\n",
    "                or df_expanded.at[prev_idx, \"index\"] != group_id\n",
    "            ):\n",
    "                break\n",
    "\n",
    "            selected_indices.add(prev_idx)\n",
    "            df_expanded.at[prev_idx, \"event\"] = label\n",
    "\n",
    "        # Add 5 frames AFTER\n",
    "        for offset in range(0, window/2):\n",
    "            next_idx = idx + offset\n",
    "            if next_idx >= len(df_expanded):\n",
    "                break\n",
    "\n",
    "            if (\n",
    "                pd.notnull(df_expanded.at[next_idx, \"event\"])\n",
    "                or df_expanded.at[next_idx, \"index\"] != group_id\n",
    "            ):\n",
    "                break\n",
    "\n",
    "            selected_indices.add(next_idx)\n",
    "            df_expanded.at[next_idx, \"event\"] = label\n",
    "\n",
    "    # Step 2: Annotate null event segments in window-frame \"neither\" batches\n",
    "    null_indices = df_expanded[df_expanded[\"event\"].isnull()].index\n",
    "    null_indices = sorted(null_indices)\n",
    "\n",
    "    i = 0\n",
    "    while i < len(null_indices):\n",
    "        if neither_count >= MAX_NEITHER_SEGMENTS:\n",
    "            break\n",
    "\n",
    "        start_idx = null_indices[i]\n",
    "        group_id = df_expanded.at[start_idx, \"index\"]\n",
    "\n",
    "        # Collect consecutive nulls in same group\n",
    "        segment = [start_idx]\n",
    "        for j in range(i + 1, len(null_indices)):\n",
    "            current_idx = null_indices[j]\n",
    "            prev_idx = null_indices[j - 1]\n",
    "\n",
    "            if (\n",
    "                current_idx == prev_idx + 1\n",
    "                and df_expanded.at[current_idx, \"index\"] == group_id\n",
    "            ):\n",
    "                segment.append(current_idx)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # Process in batches of window\n",
    "        for k in range(0, len(segment), window):\n",
    "            if neither_count >= MAX_NEITHER_SEGMENTS:\n",
    "                break\n",
    "\n",
    "            batch = segment[k : k + window]\n",
    "            if len(batch) == window:\n",
    "                for idx in batch:\n",
    "                    selected_indices.add(idx)\n",
    "                    df_expanded.at[idx, \"event\"] = \"neither\"\n",
    "                neither_count += 1\n",
    "\n",
    "        i += len(segment)\n",
    "\n",
    "    # Step 3: Final filtering\n",
    "    df_expanded = df_expanded.loc[sorted(selected_indices)].reset_index(drop=True)\n",
    "\n",
    "    print(f\"Number of 'neither' segments (window-frame batches): {neither_count}\")\n",
    "\n",
    "    return df_expanded\n",
    "\n",
    "\n",
    "train_df_expanded = add_sequence(train_df_expanded)\n",
    "test_df_expanded = add_sequence(test_df_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c968bf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_expanded[\"event\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540897b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_expanded[\"event\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e115a6",
   "metadata": {},
   "source": [
    "## Better Approach (LSTM - 30 Frames per event)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98163516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491f1b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_df = pd.read_csv(\n",
    "    \"/home/g03-s2025/Desktop/SquashCoachingCopilot/cv-module/digitization/event-recognition/stroke-detection/implementation/annotated_jsons/combined_data.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0ed066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------------\n",
    "# # Step 1: Normalize coordinates\n",
    "# # ----------------------------\n",
    "# coord_cols = [\n",
    "#     col\n",
    "#     for col in train_df_expanded.columns\n",
    "#     if col.startswith(\"x\") or col.startswith(\"y\")\n",
    "# ]\n",
    "# scaler = MinMaxScaler()\n",
    "# train_df_expanded[coord_cols] = scaler.fit_transform(train_df_expanded[coord_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23741751",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_expanded.drop(columns=[\"player_id\", \"time\", \"index\"], inplace=True)\n",
    "test_df_expanded.drop(columns=[\"player_id\", \"time\", \"index\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5017726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Label encoding\n",
    "# ----------------------------\n",
    "label_map = {\"forehand\": 0, \"backhand\": 1, \"neither\": 2}\n",
    "train_df_expanded = train_df_expanded[\n",
    "    train_df_expanded[\"event\"].isin(label_map.keys())\n",
    "].copy()\n",
    "train_df_expanded[\"event\"] = train_df_expanded[\"event\"].map(label_map).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3fb99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------------\n",
    "# # Step 1: Normalize coordinates\n",
    "# # ----------------------------\n",
    "# coord_cols = [\n",
    "#     col\n",
    "#     for col in test_df_expanded.columns\n",
    "#     if col.startswith(\"x\") or col.startswith(\"y\")\n",
    "# ]\n",
    "# scaler = MinMaxScaler()\n",
    "# test_df_expanded[coord_cols] = scaler.fit_transform(test_df_expanded[coord_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e639e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_cols = [\n",
    "    col\n",
    "    for col in test_df_expanded.columns\n",
    "    if col.startswith(\"x\") or col.startswith(\"y\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab171532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Label encoding\n",
    "# ----------------------------\n",
    "label_map = {\"forehand\": 0, \"backhand\": 1, \"neither\": 2}\n",
    "test_df_expanded = test_df_expanded[\n",
    "    test_df_expanded[\"event\"].isin(label_map.keys())\n",
    "].copy()\n",
    "test_df_expanded[\"event\"] = test_df_expanded[\"event\"].map(label_map).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6132461f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation with non-overlapping windows\n",
    "X_train = []\n",
    "y_train = []\n",
    "window_size = 15\n",
    "\n",
    "# Process the data in non-overlapping chunks of size 16\n",
    "for i in range(0, len(train_df_expanded), window_size):\n",
    "    # Check if we have a full window\n",
    "    if i + window_size <= len(train_df_expanded):\n",
    "        window = train_df_expanded.iloc[i : i + window_size]\n",
    "\n",
    "        # Only use the window if all rows have the same event\n",
    "        if window[\"event\"].nunique() == 1:\n",
    "            X_train.append(\n",
    "                window[coord_cols].values\n",
    "            )  # Shape: (16, 24) - 16 timesteps, 24 features\n",
    "            # Store the single event for this window\n",
    "            y_train.append(window[\"event\"].iloc[0])\n",
    "\n",
    "X_train = np.array(X_train)  # shape should be (num_non_overlapping_windows, 16, 24)\n",
    "y_train = np.array(y_train)  # shape should be (num_non_overlapping_windows,)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"Label distribution: {np.unique(y_train, return_counts=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6066bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation with non-overlapping windows\n",
    "X_test = []\n",
    "y_test = []\n",
    "window_size = 15\n",
    "\n",
    "# Process the data in non-overlapping chunks of size 16\n",
    "for i in range(0, len(test_df_expanded), window_size):\n",
    "    # Check if we have a full window\n",
    "    if i + window_size <= len(test_df_expanded):\n",
    "        window = test_df_expanded.iloc[i : i + window_size]\n",
    "\n",
    "        # Only use the window if all rows have the same event\n",
    "        if window[\"event\"].nunique() == 1:\n",
    "            X_test.append(\n",
    "                window[coord_cols].values\n",
    "            )  # Shape: (16, 24) - 16 timesteps, 24 features\n",
    "            # Store the single event for this window\n",
    "            y_test.append(window[\"event\"].iloc[0])\n",
    "\n",
    "X_test = np.array(X_test)  # shape should be (num_non_overlapping_windows, 16, 24)\n",
    "y_test = np.array(y_test)  # shape should be (num_non_overlapping_windows,)\n",
    "\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print(f\"Label distribution: {np.unique(y_test, return_counts=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f98a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(\n",
    "#     X, y, test_size=0.2, random_state=42, stratify=y\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c564920",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Input shape: (16 timesteps, 24 features)\n",
    "model.add(LSTM(16, input_shape=(window_size, len(coord_cols)), return_sequences=False))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(3, activation=\"softmax\"))  # 3 classes (0, 1, 2)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a4c35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Step 6: Training\n",
    "# ----------------------------\n",
    "# Class weights\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Don't use SMOTE, use class weights instead\n",
    "class_weights = compute_class_weight(\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "print(f\"Class weights: {class_weight_dict}\")\n",
    "# Example: {0: 0.5, 1: 5.0, 2: 5.0}  # penalizes misclassifying minority classes\n",
    "\n",
    "# Early stopping\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=8,  # You can adjust this batch size for training\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1,\n",
    "    class_weight=class_weight_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881f3d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print classification report\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff80162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and validation accuracy histories\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_acc = history.history[\"accuracy\"]\n",
    "val_acc = history.history[\"val_accuracy\"]\n",
    "\n",
    "EPOCHS = len(training_acc)\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, EPOCHS + 1)\n",
    "\n",
    "# Visualize accuracy history\n",
    "plt.figure()\n",
    "plt.plot(epoch_count, training_acc, \"r--\")\n",
    "plt.plot(epoch_count, val_acc, \"b-\")\n",
    "plt.legend([\"LSTM Training Accuracy\", \"LSTM Val Accuracy\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"LSTM Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0365528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df_expanded[coord_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178e0d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = train_df_expanded.drop(columns=[\"event\"])\n",
    "# y = train_df_expanded[\"event\"]\n",
    "\n",
    "# test_x = test_df_expanded.drop(columns=[\"event\"])\n",
    "# test_y = test_df_expanded[\"event\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ab9781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import xgboost\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# xgb = XGBClassifier()\n",
    "# xgb.fit(X, y)\n",
    "# y_pred_xgb = xgb.predict(test_x)\n",
    "# print(classification_report(test_y, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b242c170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison_df = pd.DataFrame(\n",
    "#     {\n",
    "#         \"y_true\": test_y,\n",
    "#         \"y_pred\": y_pred_xgb,\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4280ec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save lstm model\n",
    "model.save(\"lstm_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
